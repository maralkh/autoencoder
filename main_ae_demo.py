"""
Main Autoencoder Demo
====================

Comprehensive demo showcasing all autoencoder types
"""

import torch
import torch.nn as nn
import argparse
import sys
import os

# Import all autoencoder implementations
from basic_autoencoders.vanilla_autoencoder import VanillaAutoencoder, AutoencoderTrainer
from basic_autoencoders.denoising_autoencoder import DenoisingAutoencoder, DenoisingTrainer
from basic_autoencoders.sparse_autoencoder import SparseAutoencoder, SparseTrainer
from basic_autoencoders.contractive_autoencoder import ContractiveAutoencoder, ContractiveTrainer
from variational_autoencoders.vanilla_vae import VanillaVAE, VAETrainer
from variational_autoencoders.beta_vae import BetaVAE, BetaVAETrainer
from image_autoencoders.convolutional_ae import ConvolutionalAutoencoder, ConvAutoencoderTrainer
from text_autoencoders.text_autoencoder import TextAutoencoder, TextAutoencoderTrainer, get_text_data_loaders
from utils.data_loaders import get_mnist_loaders, get_fashion_mnist_loaders, get_cifar10_loaders
from utils.visualization import AutoencoderVisualizer
from utils.latent_space_analyzer import LatentSpaceAnalyzer, compare_latent_spaces


class AutoencoderDemo:
    """Main demo class for all autoencoder types"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ Autoencoder Demo initialized on {self.device}")
    
    def run_vanilla_autoencoder_demo(self, epochs=10, batch_size=128):
        """Run Vanilla Autoencoder demo"""
        print("\n" + "="*60)
        print("ğŸ”¥ VANILLA AUTOENCODER DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=batch_size, train_size=5000, test_size=1000)
        
        # Create model
        model = VanillaAutoencoder(
            input_dim=784,
            hidden_dims=[512, 256, 128],
            latent_dim=64
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = AutoencoderTrainer(model, self.device)
        trainer.train(train_loader, test_loader, epochs=epochs)
        
        # Visualize
        visualizer = AutoencoderVisualizer(model, self.device)
        visualizer.visualize_reconstructions(test_loader)
        visualizer.visualize_latent_space(test_loader)
        
        # Comprehensive latent space analysis
        print("ğŸ”¬ Performing comprehensive latent space analysis...")
        latent_analyzer = LatentSpaceAnalyzer(model, self.device)
        latent_analyzer.extract_latent_representations(test_loader, max_samples=2000)
        latent_report = latent_analyzer.generate_comprehensive_report()
        
        return model, trainer
    
    def run_denoising_autoencoder_demo(self, epochs=10, batch_size=128):
        """Run Denoising Autoencoder demo"""
        print("\n" + "="*60)
        print("ğŸ§¹ DENOISING AUTOENCODER DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=batch_size, train_size=5000, test_size=1000)
        
        # Create model
        model = DenoisingAutoencoder(
            input_dim=784,
            hidden_dims=[512, 256, 128],
            latent_dim=64,
            dropout_rate=0.2
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = DenoisingTrainer(model, self.device)
        trainer.train(train_loader, test_loader, epochs=epochs, 
                     noise_type='gaussian', noise_factor=0.3)
        
        # Visualize denoising
        from basic_autoencoders.denoising_autoencoder import visualize_denoising
        visualize_denoising(model, test_loader, self.device)
        
        return model, trainer
    
    def run_sparse_autoencoder_demo(self, epochs=10, batch_size=128):
        """Run Sparse Autoencoder demo"""
        print("\n" + "="*60)
        print("âœ¨ SPARSE AUTOENCODER DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=batch_size, train_size=5000, test_size=1000)
        
        # Create model
        model = SparseAutoencoder(
            input_dim=784,
            hidden_dims=[512, 256, 128],
            latent_dim=64,
            sparsity_target=0.05,
            sparsity_weight=3.0
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = SparseTrainer(model, self.device)
        trainer.train(train_loader, test_loader, epochs=epochs)
        
        # Analyze sparsity
        from basic_autoencoders.sparse_autoencoder import analyze_sparsity
        analyze_sparsity(model, test_loader, self.device)
        
        return model, trainer
    
    def run_vae_demo(self, epochs=10, batch_size=128):
        """Run Variational Autoencoder demo"""
        print("\n" + "="*60)
        print("ğŸ¯ VARIATIONAL AUTOENCODER DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=batch_size, train_size=5000, test_size=1000, normalize=False)
        
        # Create model
        model = VanillaVAE(
            input_dim=784,
            hidden_dims=[512, 256],
            latent_dim=20
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = VAETrainer(model, self.device, beta=1.0)
        trainer.train(train_loader, test_loader, epochs=epochs)
        
        # Visualize
        from variational_autoencoders.vanilla_vae import visualize_generation, latent_space_interpolation
        visualize_generation(model, self.device)
        latent_space_interpolation(model, test_loader, self.device)
        
        return model, trainer
    
    def run_beta_vae_demo(self, epochs=10, batch_size=128):
        """Run Beta-VAE demo"""
        print("\n" + "="*60)
        print("âš–ï¸ BETA-VAE DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=batch_size, train_size=5000, test_size=1000, normalize=False)
        
        # Create model
        model = BetaVAE(
            input_dim=784,
            hidden_dims=[400, 200],
            latent_dim=10,
            beta=4.0
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = BetaVAETrainer(model, self.device)
        trainer.train(train_loader, test_loader, epochs=epochs)
        
        # Analyze disentanglement
        from variational_autoencoders.beta_vae import analyze_disentanglement, visualize_latent_traversal
        analyze_disentanglement(model, test_loader, self.device, num_samples=500)
        
        # Show latent traversals
        for dim in range(min(3, model.latent_dim)):
            visualize_latent_traversal(model, self.device, latent_dim_idx=dim)
        
        return model, trainer
    
    def run_convolutional_ae_demo(self, epochs=10, batch_size=128):
        """Run Convolutional Autoencoder demo"""
        print("\n" + "="*60)
        print("ğŸ–¼ï¸ CONVOLUTIONAL AUTOENCODER DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=batch_size, train_size=5000, test_size=1000, normalize=False)
        
        # Create model
        model = ConvolutionalAutoencoder(
            input_channels=1,
            latent_dim=128,
            image_size=28
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = ConvAutoencoderTrainer(model, self.device)
        trainer.train(train_loader, test_loader, epochs=epochs)
        
        # Visualize
        from image_autoencoders.convolutional_ae import visualize_feature_maps, analyze_latent_space
        visualize_feature_maps(model, test_loader, self.device, layer_idx=0)
        analyze_latent_space(model, test_loader, self.device)
        
        return model, trainer
    
    def run_text_autoencoder_demo(self, epochs=10, batch_size=32):
        """Run Text Autoencoder demo"""
        print("\n" + "="*60)
        print("ğŸ“ TEXT AUTOENCODER DEMO")
        print("="*60)
        
        # Generate sample text data
        from text_autoencoders.text_autoencoder import generate_sample_texts
        texts = generate_sample_texts(1000)
        
        # Create data loaders
        train_loader, val_loader, dataset = get_text_data_loaders(
            texts, batch_size=batch_size, max_length=15
        )
        
        print(f"Vocabulary size: {dataset.vocab_size}")
        
        # Create model
        model = TextAutoencoder(
            vocab_size=dataset.vocab_size,
            embedding_dim=64,
            hidden_dim=128,
            latent_dim=32,
            num_layers=2
        )
        
        print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
        
        # Train
        trainer = TextAutoencoderTrainer(model, self.device)
        trainer.train(train_loader, val_loader, epochs=epochs)
        
        # Visualize
        from text_autoencoders.text_autoencoder import visualize_text_reconstructions, text_interpolation
        visualize_text_reconstructions(model, dataset, val_loader)
        
        # Text interpolation
        text1 = "artificial intelligence is powerful"
        text2 = "machine learning models are useful"
        text_interpolation(model, dataset, text1, text2)
        
        return model, trainer
    
    def run_comparison_demo(self, epochs=5):
        """Run comparison of multiple autoencoder types"""
        print("\n" + "="*60)
        print("ğŸ“Š AUTOENCODER COMPARISON DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=128, train_size=2000, test_size=500)
        
        models = {}
        trainers = {}
        
        # Train different models
        autoencoder_configs = [
            ('Vanilla AE', VanillaAutoencoder, AutoencoderTrainer, 
             {'input_dim': 784, 'hidden_dims': [256, 128], 'latent_dim': 32}),
            ('Sparse AE', SparseAutoencoder, SparseTrainer,
             {'input_dim': 784, 'hidden_dims': [256, 128], 'latent_dim': 32, 'sparsity_target': 0.1}),
            ('Denoising AE', DenoisingAutoencoder, DenoisingTrainer,
             {'input_dim': 784, 'hidden_dims': [256, 128], 'latent_dim': 32})
        ]
        
        for name, model_class, trainer_class, config in autoencoder_configs:
            print(f"\nğŸ§  Training {name}...")
            model = model_class(**config)
            trainer = trainer_class(model, self.device)
            trainer.train(train_loader, test_loader, epochs=epochs)
            
            models[name] = model
            trainers[name] = trainer
        
        # Compare results
        from utils.visualization import create_comparison_plot
        create_comparison_plot(models, test_loader, metric='reconstruction_error')
        
        # Comprehensive latent space comparison
        print("ğŸ”¬ Performing comprehensive latent space comparison...")
        latent_comparison = compare_latent_spaces(models, test_loader, self.device, max_samples=1000)
        
        # Plot all training curves
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 8))
        for name, trainer in trainers.items():
            plt.plot(trainer.train_losses, label=f'{name} Train', linewidth=2)
            plt.plot(trainer.val_losses, label=f'{name} Val', linestyle='--', linewidth=2)
        
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Comparison')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()
        
        return models, trainers
    
    def run_interactive_demo(self):
        """Interactive demo allowing user to choose autoencoder type"""
        print("\n" + "="*60)
        print("ğŸ® INTERACTIVE AUTOENCODER DEMO")
        print("="*60)
        
        demos = {
            '1': ('Vanilla Autoencoder', self.run_vanilla_autoencoder_demo),
            '2': ('Denoising Autoencoder', self.run_denoising_autoencoder_demo),
            '3': ('Sparse Autoencoder', self.run_sparse_autoencoder_demo),
            '4': ('Variational Autoencoder (VAE)', self.run_vae_demo),
            '5': ('Beta-VAE', self.run_beta_vae_demo),
            '6': ('Convolutional Autoencoder', self.run_convolutional_ae_demo),
            '7': ('Text Autoencoder', self.run_text_autoencoder_demo),
            '8': ('Comparison Demo', self.run_comparison_demo),
            '9': ('Latent Space Analysis Demo', self.run_latent_analysis_demo),
            '10': ('Transformer Attention Demo', self.run_transformer_demo),
            '11': ('Run All Demos', self.run_all_demos)
        }
        
        while True:
            print("\nChoose an autoencoder demo:")
            for key, (name, _) in demos.items():
                print(f"{key}. {name}")
            print("0. Exit")
            
            choice = input("\nEnter your choice (0-11): ").strip()
            
            if choice == '0':
                print("ğŸ‘‹ Goodbye!")
                break
            elif choice in demos:
                name, demo_func = demos[choice]
                print(f"\nğŸš€ Running {name}...")
                try:
                    demo_func()
                    print(f"âœ… {name} completed successfully!")
                except Exception as e:
                    print(f"âŒ Error in {name}: {e}")
            else:
                print("âŒ Invalid choice. Please try again.")
    
    def run_all_demos(self, quick_mode=True):
        """Run all demos sequentially"""
        print("\n" + "="*60)
        print("ğŸƒ RUNNING ALL AUTOENCODER DEMOS")
        print("="*60)
        
        epochs = 3 if quick_mode else 10
        batch_size = 256 if quick_mode else 128
        
        print(f"Quick mode: {quick_mode} (epochs: {epochs}, batch_size: {batch_size})")
        
        results = {}
        
        try:
            print("\n1ï¸âƒ£ Vanilla Autoencoder...")
            results['vanilla'] = self.run_vanilla_autoencoder_demo(epochs, batch_size)
        except Exception as e:
            print(f"âŒ Vanilla AE failed: {e}")
        
        try:
            print("\n2ï¸âƒ£ Denoising Autoencoder...")
            results['denoising'] = self.run_denoising_autoencoder_demo(epochs, batch_size)
        except Exception as e:
            print(f"âŒ Denoising AE failed: {e}")
        
        try:
            print("\n3ï¸âƒ£ Sparse Autoencoder...")
            results['sparse'] = self.run_sparse_autoencoder_demo(epochs, batch_size)
        except Exception as e:
            print(f"âŒ Sparse AE failed: {e}")
        
        try:
            print("\n4ï¸âƒ£ Variational Autoencoder...")
            results['vae'] = self.run_vae_demo(epochs, batch_size)
        except Exception as e:
            print(f"âŒ VAE failed: {e}")
        
        try:
            print("\n5ï¸âƒ£ Beta-VAE...")
            results['beta_vae'] = self.run_beta_vae_demo(epochs, batch_size)
        except Exception as e:
            print(f"âŒ Beta-VAE failed: {e}")
        
        try:
            print("\n6ï¸âƒ£ Convolutional Autoencoder...")
            results['conv'] = self.run_convolutional_ae_demo(epochs, batch_size)
        except Exception as e:
            print(f"âŒ Conv AE failed: {e}")
        
        try:
            print("\n7ï¸âƒ£ Text Autoencoder...")
            results['text'] = self.run_text_autoencoder_demo(epochs, 32)
        except Exception as e:
            print(f"âŒ Text AE failed: {e}")
        
        print(f"\nâœ… Completed {len(results)} out of 7 demos!")
        return results
    
    def run_latent_analysis_demo(self, epochs=10):
        """Run dedicated latent space analysis demo"""
        print("\n" + "="*60)
        print("ğŸ”¬ LATENT SPACE ANALYSIS DEMO")
        print("="*60)
        
        # Load data
        train_loader, test_loader = get_mnist_loaders(batch_size=128, train_size=3000, test_size=1000)
        
        print("Training multiple models for latent space comparison...")
        
        models = {}
        
        # Train different autoencoder types for comparison
        configs = [
            ('Vanilla AE', VanillaAutoencoder, AutoencoderTrainer, 
             {'input_dim': 784, 'hidden_dims': [256, 128], 'latent_dim': 32}),
            ('VAE', VanillaVAE, VAETrainer,
             {'input_dim': 784, 'hidden_dims': [256, 128], 'latent_dim': 32}),
            ('Î²-VAE', BetaVAE, BetaVAETrainer,
             {'input_dim': 784, 'hidden_dims': [256, 128], 'latent_dim': 32, 'beta': 4.0})
        ]
        
        for name, model_class, trainer_class, config in configs:
            print(f"\nğŸ§  Training {name}...")
            
            # Adjust data loaders for VAE (no normalization)
            if 'VAE' in name:
                train_loader_vae, test_loader_vae = get_mnist_loaders(
                    batch_size=128, train_size=3000, test_size=1000, normalize=False
                )
                model = model_class(**config)
                trainer = trainer_class(model, self.device)
                trainer.train(train_loader_vae, test_loader_vae, epochs=epochs)
                models[name] = model
            else:
                model = model_class(**config)
                trainer = trainer_class(model, self.device)
                trainer.train(train_loader, test_loader, epochs=epochs)
                models[name] = model
        
        # Comprehensive latent space analysis
        print("\nğŸ” Performing comprehensive latent space analysis...")
        
        # Individual analysis for each model
        individual_reports = {}
        for name, model in models.items():
            print(f"\nğŸ“Š Analyzing {name} latent space...")
            analyzer = LatentSpaceAnalyzer(model, self.device)
            
            # Use appropriate data loader
            if 'VAE' in name:
                analyzer.extract_latent_representations(test_loader_vae, max_samples=1000)
            else:
                analyzer.extract_latent_representations(test_loader, max_samples=1000)
            
            report = analyzer.generate_comprehensive_report()
            individual_reports[name] = report
        
        # Comparative analysis
        print("\nâš–ï¸ Performing comparative latent space analysis...")
        comparison_results = compare_latent_spaces(models, test_loader, self.device, max_samples=1000)
        
        # Detailed analysis for best model
        best_model_name = max(comparison_results.keys(), 
                             key=lambda x: comparison_results[x]['dim_analysis']['utilization_ratio'])
        
        print(f"\nğŸ† Detailed analysis of best model: {best_model_name}")
        best_analyzer = comparison_results[best_model_name]['analyzer']
        
        # Advanced analyses
        print("ğŸ¯ Performing advanced analyses...")
        
        # Interpolation analysis
        interpolation_results = best_analyzer.interpolation_analysis(num_pairs=8, num_steps=10)
        print(f"   âœ… Interpolation analysis: {interpolation_results['average_smoothness']:.4f} smoothness")
        
        # Neighborhood analysis  
        neighborhood_results = best_analyzer.neighborhood_analysis(k=10)
        print(f"   âœ… Neighborhood analysis: {neighborhood_results['average_purity']:.3f} purity")
        
        # Manifold analysis
        manifold_results = best_analyzer.manifold_analysis()
        print(f"   âœ… Manifold analysis: {manifold_results['estimated_intrinsic_dimension']:.2f} intrinsic dim")
        
        print("\nâœ¨ Latent space analysis completed!")
        print("ğŸ’¡ Key insights:")
        print(f"   â€¢ Best model: {best_model_name}")
        print(f"   â€¢ Utilization: {comparison_results[best_model_name]['dim_analysis']['utilization_ratio']:.1%}")
        print(f"   â€¢ Independence: {comparison_results[best_model_name]['disentanglement']['independence_score']:.3f}")
        
        return models, individual_reports, comparison_results
    
    def run_transformer_demo(self):
        """Run Transformer attention analysis demo"""
        print("\n" + "="*60)
        print("ğŸ¤– TRANSFORMER ATTENTION ANALYSIS DEMO")
        print("="*60)
        
        try:
            # Check if transformers is available
            import transformers
            print(f"âœ… Transformers library available (v{transformers.__version__})")
        except ImportError:
            print("âŒ Transformers library not found!")
            print("ğŸ’¡ Install with: pip install transformers")
            return None
        
        try:
            from transformer_autoencoders.transformer_autoencoder import TransformerRunner
            from transformer_autoencoders.attention_data_analyzer import AttentionDataAnalyzer
        except ImportError:
            print("âŒ Transformer modules not found!")
            print("ğŸ’¡ Make sure transformer_autoencoders folder exists")
            return None
        
        # Sample texts for analysis
        sample_texts = [
            "The transformer architecture revolutionized natural language processing through self-attention mechanisms.",
            "Attention allows models to focus on relevant parts of the input when making predictions.",
            "Deep learning models require large datasets and significant computational resources for training.",
            "Neural networks learn complex patterns by adjusting weights through backpropagation algorithms.",
            "Machine learning has applications in computer vision, natural language processing, and robotics."
        ]
        
        print(f"ğŸ“ Prepared {len(sample_texts)} sample texts for analysis")
        
        try:
            # Initialize transformer runner
            print("\nğŸ”„ Initializing transformer model...")
            runner = TransformerRunner(model_name="gpt2", device=self.device)
            
            # Prepare data
            print("ğŸ“Š Preparing input data...")
            input_ids, attention_mask = runner.prepare_data(sample_texts, max_length=32)
            print(f"âœ… Input prepared: {input_ids.shape}")
            
            # Run with attention hooks
            print("ğŸ”— Running model with attention capture...")
            save_dir = "demo_transformer_attention"
            outputs = runner.run_with_hooks(input_ids, attention_mask, save_dir)
            
            print(f"âœ… Captured attention data:")
            print(f"   â€¢ Attention inputs: {len(runner.attention_hook.attention_inputs)}")
            print(f"   â€¢ Attention outputs: {len(runner.attention_hook.attention_outputs)}")
            print(f"   â€¢ Attention weights: {len(runner.attention_hook.attention_weights)}")
            
            # Analyze patterns
            print("\nğŸ“Š Analyzing attention patterns...")
            analysis = runner.analyze_attention_patterns(save_dir)
            
            # Create visualizations
            print("ğŸ¨ Creating attention visualizations...")
            runner.visualize_attention_flow(save_dir)
            
            # Visualize attention weights for first layer/head
            if runner.attention_hook.attention_weights:
                print("ğŸ¯ Creating attention weight visualizations...")
                runner.visualize_attention_weights(save_dir, layer_idx=0, head_idx=0)
                if len(runner.attention_hook.attention_weights) > 1:
                    runner.visualize_attention_weights(save_dir, layer_idx=1, head_idx=0)
            
            # Comprehensive analysis
            print("ğŸ“‹ Creating comprehensive analysis report...")
            analyzer = AttentionDataAnalyzer(save_dir)
            analyzer.create_comprehensive_report()
            
            print(f"\nâœ… Transformer attention analysis completed!")
            print(f"ğŸ“ All results saved to: {save_dir}")
            print(f"ğŸ” Check the directory for:")
            print(f"   â€¢ Attention data files (.pkl)")
            print(f"   â€¢ Visualization plots (.png)")
            print(f"   â€¢ Analysis reports (.txt, .json)")
            
            return {
                'runner': runner,
                'analyzer': analyzer,
                'save_dir': save_dir,
                'analysis': analysis
            }
            
        except Exception as e:
            print(f"âŒ Error during transformer analysis: {e}")
            print("ğŸ’¡ Make sure you have sufficient memory and the model can be loaded")
            return None


def create_project_structure():
    """Create the project directory structure"""
    dirs = [
        'basic_autoencoders',
        'variational_autoencoders', 
        'image_autoencoders',
        'text_autoencoders',
        'audio_autoencoders',
        'multimodal_autoencoders',
        'utils',
        'data',
        'models',
        'results'
    ]
    
    for dir_name in dirs:
        os.makedirs(dir_name, exist_ok=True)
        
        # Create __init__.py files
        init_file = os.path.join(dir_name, '__init__.py')
        if not os.path.exists(init_file):
            with open(init_file, 'w') as f:
                f.write(f'"""{"="*50}\n{dir_name.replace("_", " ").title()} Module\n{"="*50}\n"""\n')
    
    print("ğŸ“ Project structure created!")


def print_repo_summary():
    """Print a summary of the repository"""
    print("\n" + "="*80)
    print("ğŸ§  AUTOENCODER REPOSITORY SUMMARY")
    print("="*80)
    
    summary = """
    ğŸ“š This repository contains comprehensive implementations of various autoencoder types:
    
    ğŸ”¥ BASIC AUTOENCODERS:
    â”œâ”€â”€ Vanilla Autoencoder - Simple encoder-decoder architecture
    â”œâ”€â”€ Denoising Autoencoder - Learns to remove noise from corrupted inputs
    â”œâ”€â”€ Sparse Autoencoder - Enforces sparsity constraints on latent representations
    â””â”€â”€ Contractive Autoencoder - Uses Jacobian regularization for robustness
    
    ğŸ¯ VARIATIONAL AUTOENCODERS:
    â”œâ”€â”€ Vanilla VAE - Probabilistic autoencoder with KL divergence regularization
    â”œâ”€â”€ Î²-VAE - VAE with controllable disentanglement via beta parameter
    â”œâ”€â”€ Conditional VAE - VAE with conditional generation capabilities
    â””â”€â”€ VQ-VAE - Vector Quantized VAE for discrete latent representations
    
    ğŸ–¼ï¸ IMAGE AUTOENCODERS:
    â”œâ”€â”€ Convolutional Autoencoder - Uses conv/deconv layers for images
    â”œâ”€â”€ U-Net Autoencoder - Skip connections for better reconstruction
    â””â”€â”€ Image Colorization AE - Converts grayscale to color images
    
    ğŸ“ TEXT AUTOENCODERS:
    â”œâ”€â”€ Text Autoencoder - RNN/LSTM based for sequential text data
    â”œâ”€â”€ Sequence-to-Sequence AE - Advanced seq2seq architecture
    â””â”€â”€ Transformer Autoencoder - Uses attention mechanisms
    
    ğŸ”Š AUDIO AUTOENCODERS:
    â”œâ”€â”€ Audio Autoencoder - For raw audio waveforms
    â””â”€â”€ Spectrogram Autoencoder - For frequency domain representations
    
    ğŸŒ MULTIMODAL AUTOENCODERS:
    â”œâ”€â”€ Multimodal VAE - Handles multiple data types simultaneously
    â””â”€â”€ Cross-Modal Autoencoder - Translates between different modalities
    
    ğŸ› ï¸ UTILITIES:
    â”œâ”€â”€ Data Loaders - Common datasets and preprocessing utilities
    â”œâ”€â”€ Visualization - Comprehensive plotting and analysis tools
    â””â”€â”€ Training Utils - Common training loops and evaluation metrics
    
    âœ¨ FEATURES:
    â€¢ Complete implementations with detailed documentation
    â€¢ Comprehensive visualization and analysis tools
    â€¢ Multiple dataset support (MNIST, CIFAR-10, text, audio)
    â€¢ GPU acceleration support
    â€¢ Modular and extensible design
    â€¢ Interactive demos and comparisons
    â€¢ Pre-trained model saving/loading
    â€¢ Extensive hyperparameter exploration
    """
    
    print(summary)
    print("="*80)


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Autoencoder Repository Demo')
    parser.add_argument('--demo', type=str, choices=[
        'vanilla', 'denoising', 'sparse', 'contractive', 'vae', 'beta_vae', 
        'conv', 'text', 'comparison', 'latent_analysis', 'transformer', 'interactive', 'all'
    ], default='interactive', help='Type of demo to run')
    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size')
    parser.add_argument('--quick', action='store_true', help='Run in quick mode (fewer epochs)')
    parser.add_argument('--setup', action='store_true', help='Setup project structure')
    parser.add_argument('--summary', action='store_true', help='Show repository summary')
    
    args = parser.parse_args()
    
    # Print header
    print("="*80)
    print("ğŸš€ WELCOME TO THE AUTOENCODER REPOSITORY!")
    print("="*80)
    print("A comprehensive collection of autoencoder implementations")
    print("for various modalities and use cases.")
    print("="*80)
    
    # Setup project structure if requested
    if args.setup:
        create_project_structure()
        return
    
    # Show summary if requested
    if args.summary:
        print_repo_summary()
        return
    
    # Initialize demo
    demo = AutoencoderDemo()
    
    # Run specified demo
    if args.demo == 'interactive':
        demo.run_interactive_demo()
    elif args.demo == 'all':
        demo.run_all_demos(quick_mode=args.quick)
    elif args.demo == 'vanilla':
        demo.run_vanilla_autoencoder_demo(args.epochs, args.batch_size)
    elif args.demo == 'denoising':
        demo.run_denoising_autoencoder_demo(args.epochs, args.batch_size)
    elif args.demo == 'sparse':
        demo.run_sparse_autoencoder_demo(args.epochs, args.batch_size)
    elif args.demo == 'vae':
        demo.run_vae_demo(args.epochs, args.batch_size)
    elif args.demo == 'beta_vae':
        demo.run_beta_vae_demo(args.epochs, args.batch_size)
    elif args.demo == 'conv':
        demo.run_convolutional_ae_demo(args.epochs, args.batch_size)
    elif args.demo == 'text':
        demo.run_text_autoencoder_demo(args.epochs, args.batch_size)
    elif args.demo == 'comparison':
        demo.run_comparison_demo(args.epochs)
    elif args.demo == 'latent_analysis':
        demo.run_latent_analysis_demo(args.epochs)
    elif args.demo == 'transformer':
        demo.run_transformer_demo()
    
    print("\nğŸ‰ Demo completed! Thank you for exploring autoencoders!")
    print("ğŸ’¡ Tip: Try different demos to see various autoencoder capabilities.")


if __name__ == "__main__":
    main()